{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports\n",
   "id": "a7433b0da820cdd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T11:05:40.718883Z",
     "start_time": "2024-12-16T11:05:35.853742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1"
   ],
   "id": "792f786235137593",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emhagama/Documents/Masters/23-24_CE901-SL_CE902-SU_mhagama_emanuel_m/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Feature Matrices",
   "id": "3e66308dfb131e0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T11:05:55.072759Z",
     "start_time": "2024-12-16T11:05:55.066711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_csv_to_4d_np(name):\n",
    "    \"\"\" Reads the csv file and changes it to 4D numpy array \"\"\"\n",
    "\n",
    "    print('{} : Reading 4D numpy {}'.format(datetime.now(), name))\n",
    "    reshaped = []\n",
    "    try:\n",
    "        np_data = np.loadtxt('{}.csv'.format(name), delimiter=',')\n",
    "        print('{} : Shape {}'.format(datetime.now(), np_data.shape))\n",
    "        reshaped = np_data.reshape(np_data.shape[1], 4, 62, np_data.shape[0] // 248)\n",
    "        # Transpose to reorder dimensions from (None, 4, 62, Feature) to (None, 62, Features, 4) for use in CNN\n",
    "        transposed = reshaped.transpose(0, 2, 3, 1)\n",
    "        print('{} : Read 4D numpy {} : Shape {}'.format(datetime.now(), name, transposed.shape))\n",
    "    except Exception as e:\n",
    "        print('{} : Error reading 4D numpy: {}'.format(datetime.now(), e))\n",
    "        traceback.print_exc()\n",
    "    return transposed"
   ],
   "id": "2cd9e33987a41603",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T11:06:50.649Z",
     "start_time": "2024-12-16T11:05:58.103685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pcc_matrix_8s = read_csv_to_4d_np('pcc_matrix_8s')\n",
    "pcc_matrix_12s = read_csv_to_4d_np('pcc_matrix_12s')\n",
    "\n",
    "conn_matrix_8s = read_csv_to_4d_np('conn_matrix_8s')\n",
    "conn_matrix_12s = read_csv_to_4d_np('conn_matrix_12s')\n",
    "\n",
    "pca_matrix_8s = read_csv_to_4d_np('pca_matrix_8s')\n",
    "pca_matrix_12s = read_csv_to_4d_np('pca_matrix_12s')\n",
    "sc_matrix_8s = read_csv_to_4d_np('sc_matrix_8s')\n",
    "sc_matrix_12s = read_csv_to_4d_np('sc_matrix_12s')"
   ],
   "id": "45a876f770e7b651",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-16 11:05:58.113807 : Reading 4D numpy pcc_matrix_8s\n",
      "2024-12-16 11:06:11.321509 : Shape (15376, 4275)\n",
      "2024-12-16 11:06:11.321949 : Read 4D numpy pcc_matrix_8s : Shape (4275, 62, 62, 4)\n",
      "2024-12-16 11:06:11.322484 : Reading 4D numpy pcc_matrix_12s\n",
      "2024-12-16 11:06:17.598160 : Shape (15376, 2025)\n",
      "2024-12-16 11:06:17.598516 : Read 4D numpy pcc_matrix_12s : Shape (2025, 62, 62, 4)\n",
      "2024-12-16 11:06:17.598994 : Reading 4D numpy conn_matrix_8s\n",
      "2024-12-16 11:06:25.985202 : Shape (15376, 4275)\n",
      "2024-12-16 11:06:25.985662 : Read 4D numpy conn_matrix_8s : Shape (4275, 62, 62, 4)\n",
      "2024-12-16 11:06:25.986306 : Reading 4D numpy conn_matrix_12s\n",
      "2024-12-16 11:06:29.939325 : Shape (15376, 2025)\n",
      "2024-12-16 11:06:29.939442 : Read 4D numpy conn_matrix_12s : Shape (2025, 62, 62, 4)\n",
      "2024-12-16 11:06:29.939561 : Reading 4D numpy pca_matrix_8s\n",
      "2024-12-16 11:06:43.090947 : Shape (15376, 4275)\n",
      "2024-12-16 11:06:43.091303 : Read 4D numpy pca_matrix_8s : Shape (4275, 62, 62, 4)\n",
      "2024-12-16 11:06:43.091745 : Reading 4D numpy pca_matrix_12s\n",
      "2024-12-16 11:06:49.352370 : Shape (15376, 2025)\n",
      "2024-12-16 11:06:49.352476 : Read 4D numpy pca_matrix_12s : Shape (2025, 62, 62, 4)\n",
      "2024-12-16 11:06:49.352584 : Reading 4D numpy sc_matrix_8s\n",
      "2024-12-16 11:06:50.227320 : Shape (992, 4275)\n",
      "2024-12-16 11:06:50.227436 : Read 4D numpy sc_matrix_8s : Shape (4275, 62, 4, 4)\n",
      "2024-12-16 11:06:50.227556 : Reading 4D numpy sc_matrix_12s\n",
      "2024-12-16 11:06:50.645282 : Shape (992, 2025)\n",
      "2024-12-16 11:06:50.645444 : Read 4D numpy sc_matrix_12s : Shape (2025, 62, 4, 4)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Labels",
   "id": "f078d97872858174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Since the labels are constant for trials of all participants, I can just copy them from the readme.txt.\n",
    "\n",
    "# I would also have to map the labels from -1, 0, 1 to 0, 1, 2 to fit in the NN models optimizers.\n",
    "\n",
    "labels_array = [1, 0, -1, -1, 0, 1, -1, 0, 1, 1, 0, -1, 0, 1, -1]\n",
    "\n",
    "def get_label_from_feature_set(feature_set, labels):\n",
    "    print('{} : Creating Labels Array'.format(datetime.now()))\n",
    "    no_participants = 15\n",
    "    no_labels = 15\n",
    "    no_segments = feature_set.shape[0] // (no_participants * no_labels)\n",
    "\n",
    "    # Repeat each label by the number of segments\n",
    "    repeated_labels = np.repeat(labels, no_segments)\n",
    "\n",
    "    # Repeat the entire sequence by the number of participants\n",
    "    label_array = np.tile(repeated_labels, no_participants)\n",
    "\n",
    "    # Map the labels ( -1 => 0, 0 => 1, 1 => 2 )\n",
    "    mapped_labels = np.where(label_array == -1, 0,\n",
    "                             np.where(label_array == 0, 1,\n",
    "                                      np.where(label_array == 1, 2, -1)))\n",
    "\n",
    "    print('{} : Created Labels Array : Shape {}'.format(datetime.now(), mapped_labels.shape))\n",
    "    return mapped_labels"
   ],
   "id": "f81898adf16befbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "labels_8s = get_label_from_feature_set(pcc_matrix_8s, labels_array)\n",
    "np.savetxt(f'labels_8s.csv', labels_8s, delimiter=',')\n",
    "labels_12s = get_label_from_feature_set(pcc_matrix_12s, labels_array)\n",
    "np.savetxt(f'labels_12s.csv', labels_12s, delimiter=',')"
   ],
   "id": "6e1c253f6c9adef0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNN Model",
   "id": "7203d2ff76a00d8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CNN Model\n",
    "class CNN:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = None\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Build the CNN model for feature extraction.\n",
    "        \"\"\"\n",
    "        input_layer = Input(shape=self.input_shape)\n",
    "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        x = Flatten()(x)  # Flatten the feature map for the next stage\n",
    "        self.model = Model(inputs=input_layer, outputs=x)\n",
    "        return self.model\n"
   ],
   "id": "1d3d8020f00c8d72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SAE Model",
   "id": "e23f62b4765e5e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SparseAutoencoder:\n",
    "    def __init__(self, input_dim, encoding_dim, sparsity=1e-5):\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.sparsity = sparsity\n",
    "        self.encoder = None\n",
    "\n",
    "    def build(self, input_layer):\n",
    "        \"\"\"\n",
    "        Build the sparse autoencoder layers.\n",
    "        \"\"\"\n",
    "        encoded = Dense(self.encoding_dim, activation='relu', activity_regularizer=l1(self.sparsity))(input_layer)\n",
    "        return encoded\n"
   ],
   "id": "d3d48a6edec77de4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DNN Model",
   "id": "1d744c472fa1d5a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DNN:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, input_layer):\n",
    "        \"\"\"\n",
    "        Build the DNN layers for classification.\n",
    "        \"\"\"\n",
    "        x = Dense(128, activation='relu')(input_layer)\n",
    "        x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "        output_layer = Dense(self.num_classes, activation='softmax')(x)\n",
    "        return output_layer\n"
   ],
   "id": "96fbc23e353ddbce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hybrid Model",
   "id": "f58a2251da7fe276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class HybridNeuralNetwork:\n",
    "    def __init__(self, input_shape, encoding_dim, num_classes, sparsity=1e-5, learning_rate=0.001, epochs=50):\n",
    "        self.input_shape = input_shape\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.sparsity = sparsity\n",
    "        self.model = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the hybrid neural network by combining CNN, SAE, and DNN.\n",
    "        \"\"\"\n",
    "        # Build CNN\n",
    "        cnn = CNN(self.input_shape)\n",
    "        cnn_model = cnn.build()\n",
    "\n",
    "        # Build Sparse Autoencoder\n",
    "        sae = SparseAutoencoder(input_dim=cnn_model.output_shape[1], encoding_dim=self.encoding_dim,\n",
    "                                sparsity=self.sparsity)\n",
    "        encoded = sae.build(cnn_model.output)\n",
    "\n",
    "        # Build DNN\n",
    "        dnn = DNN(self.num_classes)\n",
    "        output_layer = dnn.build(encoded)\n",
    "\n",
    "        # Combine all components into a single model\n",
    "        self.model = Model(inputs=cnn_model.input, outputs=output_layer)\n",
    "\n",
    "    def compile_model(self):\n",
    "        \"\"\"\n",
    "        Compile the model with a specified learning rate.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been built yet. Call build_model() first.\")\n",
    "        self.model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_val, y_val, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the model on the given training data.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been built yet. Call build_model() first.\")\n",
    "        history = self.model.fit(X_train, y_train,\n",
    "                                 validation_data=(X_val, y_val),\n",
    "                                 epochs=self.epochs,\n",
    "                                 batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test data.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been built yet. Call build_model() first.\")\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been built yet. Call build_model() first.\")\n",
    "        return self.model.predict(X)\n"
   ],
   "id": "96d3a0b32f839e5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting",
   "id": "54f1c8a4dadc187c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_history(history, history_type, feature_type):\n",
    "    plt.plot(history.history[feature_type.split('-')[0]], label=f'{history_type} {feature_type}')\n",
    "    plt.title(f'{history_type} {feature_type}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(feature_type.split('-')[0])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "c6be9c0f4fa5b595"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data split",
   "id": "a21baddde6daa3d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# A function to split data into 80-20 training test\n",
    "def split_data(features, labels, test_size=0.2):\n",
    "    print('{} : Splitting Data and Labels'.format(datetime.now()))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=42)\n",
    "    print(f\"Training set shape: {x_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Testing set shape: {x_test.shape}, Testing labels shape: {y_test.shape}\")\n",
    "    print('{} : Splitting Data and Labels, DONE'.format(datetime.now()))\n",
    "    return x_train, x_test, y_train, y_test"
   ],
   "id": "bd7587f47b73ced1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### History Printing",
   "id": "5ad7f2fd4f97067f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_history_average(_history, dict, feature_type):\n",
    "    # Calculate final averages\n",
    "    final_avg = {}\n",
    "    for key in _history.history.keys():\n",
    "        final_avg[key] = np.mean(_history.history[key])\n",
    "\n",
    "    # Print final averages\n",
    "    print(\"\\nFinal Averages:\")\n",
    "    for key, value in final_avg.items():\n",
    "        print(f\"{key} (average): {value:.4f}\")\n",
    "\n",
    "    dict[feature_type] = final_avg\n",
    "    return dict"
   ],
   "id": "c99649589aec7ee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parameter and Results Printing and Saving",
   "id": "61317db15a50639a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_params_and_results(model_dict, results_dict, cm_dict, report_dict, learning_rate, epochs, file_path, instance):\n",
    "    # Prepare data for tabulation\n",
    "    param_rows = []\n",
    "    for model_name, model in model_dict.items():\n",
    "        for layer in model.model.layers:\n",
    "            # Access all weights and biases for layers with parameters\n",
    "            weights = layer.get_weights()\n",
    "\n",
    "            # If layer has weights (like Dense, Conv2D)\n",
    "            if weights:\n",
    "                for i, weight_array in enumerate(weights):\n",
    "                    if i == 0:\n",
    "                        # First array in weights is the kernel (weights)\n",
    "                        param_name = f\"{layer.name}_weights\"\n",
    "                    elif i == 1:\n",
    "                        # Second array in weights is the bias\n",
    "                        param_name = f\"{layer.name}_bias\"\n",
    "\n",
    "                    param_rows.append([model_name, param_name, str(weight_array.shape), weight_array])\n",
    "\n",
    "            # Access regularizer parameters (if any)\n",
    "            if hasattr(layer, 'kernel_regularizer') and layer.kernel_regularizer is not None:\n",
    "                regularizer = layer.kernel_regularizer\n",
    "                param_rows.append([model_name, f\"{layer.name}_regularizer\", str(regularizer), 'N/A'])\n",
    "\n",
    "    # Headers for the table\n",
    "    param_headers = ['Model', 'Parameter Name', 'Shape', 'Values (First Few)']\n",
    "\n",
    "    # Function to truncate large weight arrays for display\n",
    "    def truncate_values(values, max_elements=10):\n",
    "        \"\"\"Truncate the displayed parameter values to show only the first few elements for readability.\"\"\"\n",
    "        if values.size > max_elements:\n",
    "            return str(values.flatten()[:max_elements]) + '...'\n",
    "        return str(values.flatten())\n",
    "\n",
    "    # Modify rows to truncate values for readability\n",
    "    rows_with_truncated_values = [\n",
    "        [row[0], row[1], row[2], str(truncate_values(row[3]))]\n",
    "        for row in param_rows\n",
    "    ]\n",
    "    # Display the table\n",
    "    print(tabulate(rows_with_truncated_values, headers=param_headers, tablefmt='grid'))\n",
    "\n",
    "    result_rows = []\n",
    "    feature_set_names = sorted(set(key.split('_test')[0] for key in results_dict.keys()))  # Extract unique model names\n",
    "\n",
    "    for model in feature_set_names:\n",
    "        train_metrics = results_dict.get(model, {})\n",
    "        test_metrics = results_dict.get(model + '_test', {})\n",
    "\n",
    "        # Create a row for each model, combining training and test metrics\n",
    "        row = [\n",
    "            model,\n",
    "            train_metrics.get('accuracy', ''),\n",
    "            train_metrics.get('loss', ''),\n",
    "            train_metrics.get('val_accuracy', ''),\n",
    "            train_metrics.get('val_loss', ''),\n",
    "            test_metrics.get('accuracy', ''),\n",
    "            test_metrics.get('loss', '')\n",
    "        ]\n",
    "        result_rows.append(row)\n",
    "\n",
    "    # Headers for the table\n",
    "    result_headers = ['Model', 'Train Accuracy', 'Train Loss', 'Validation Accuracy', 'Validation Loss',\n",
    "                      'Test Accuracy', 'Test Loss']\n",
    "\n",
    "    # Display table using tabulate\n",
    "    print(tabulate(result_rows, headers=result_headers, tablefmt='grid'))\n",
    "\n",
    "    # Ensure param_rows and result_rows are 2D\n",
    "    param_rows = [list(row) for row in param_rows]\n",
    "    result_rows = [list(row) for row in result_rows]\n",
    "\n",
    "    # Convert the tables to pandas DataFrames\n",
    "    param_df = pd.DataFrame(param_rows, columns=param_headers)\n",
    "    result_df = pd.DataFrame(result_rows, columns=result_headers)\n",
    "\n",
    "    # Export to an Excel file\n",
    "    if file_path == '':\n",
    "        now = datetime.now()\n",
    "        formatted_time = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "        file_path = f'./output/output_tables_{epochs}_{learning_rate}_{formatted_time}.xlsx'\n",
    "\n",
    "    # Load the existing workbook\n",
    "    sheet_name = f's_{instance}'\n",
    "\n",
    "    try:\n",
    "        # Try to load the existing workbook\n",
    "        workbook = load_workbook(file_path)\n",
    "    except FileNotFoundError:\n",
    "        # Create a new workbook if it doesn't exist\n",
    "        workbook = Workbook()\n",
    "        workbook.remove(workbook.active)  # Remove the default sheet\n",
    "\n",
    "    # Create a new sheet for the instance\n",
    "    if sheet_name in workbook.sheetnames:\n",
    "        print(f\"Sheet '{sheet_name}' already exists. Overwriting it.\")\n",
    "        workbook.remove(workbook[sheet_name])\n",
    "    worksheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "    # Write param_df to the sheet\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(param_df, index=False, header=True), start=1):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            worksheet.cell(row=r_idx, column=c_idx, value=str(value))\n",
    "\n",
    "    # Write result_df below param_df, with a blank row in between\n",
    "    start_row = len(param_df) + 3\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(result_df, index=False, header=True), start=start_row):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            worksheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    # Write confusion_matrix below result_df, with a blank row in between\n",
    "    start_column = 1\n",
    "    for cm_name, cm_df in cm_dict.items():\n",
    "        print(cm_name)\n",
    "        start_row = len(result_df) + len(param_df) + 9\n",
    "        start_column = start_column\n",
    "        worksheet.cell(row=start_row - 2, column=start_column + 2, value=cm_name)\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(cm_df, index=True, header=True), start=start_row):\n",
    "            for c_idx, value in enumerate(row, start_column):\n",
    "                worksheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "        start_column = start_column + 5\n",
    "\n",
    "    # Write classification report below confusion matrix, with a blank row in between\n",
    "    start_column = 1\n",
    "    for report_name, report_df in report_dict.items():\n",
    "        print(report_name)\n",
    "        start_row = len(result_df) + len(param_df) + 19\n",
    "        start_column = start_column\n",
    "        worksheet.cell(row=start_row - 2, column=start_column + 2, value=report_name)\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(report_df, index=True, header=True), start=start_row):\n",
    "            for c_idx, value in enumerate(row, start_column):\n",
    "                worksheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "        start_column = start_column + 6\n",
    "    # Save the workbook\n",
    "    workbook.save(file_path)\n",
    "    print(f\"Tables saved to {instance} in {file_path}.\")\n",
    "    return file_path"
   ],
   "id": "19f3d0c28a408805"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Execution",
   "id": "158fe4efe4c946a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def execute_network(features, labels, feature_type, results_dict, model_dict, learning_rate, epochs, cm_dictionary,\n",
    "                    report_dictionary):\n",
    "    print('{} : Executing Network for {} Features ****************************'.format(datetime.now(), feature_type))\n",
    "\n",
    "    # Initialize the hybrid neural network\n",
    "    hybrid_nn = HybridNeuralNetwork(input_shape=(62, 62, 4), encoding_dim=128, num_classes=3,\n",
    "                                    learning_rate=learning_rate, epochs=epochs)\n",
    "    if 'SC' in feature_type:\n",
    "        hybrid_nn = HybridNeuralNetwork(input_shape=(62, 4, 4), encoding_dim=128, num_classes=3,\n",
    "                                        learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "    # Build the model\n",
    "    hybrid_nn.build_model()\n",
    "\n",
    "    # Compile the model\n",
    "    hybrid_nn.compile_model()\n",
    "\n",
    "    # Split Data\n",
    "    X_train_1, X_test, y_train_1, y_test = split_data(features, labels)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = split_data(X_train_1, y_train_1)\n",
    "\n",
    "    # Train the model (replace X_train, y_train, X_val, y_val with actual data)\n",
    "    history = hybrid_nn.train_model(X_train, y_train, X_val, y_val, batch_size=32)\n",
    "\n",
    "    plot_history(history, 'Training and Validation ', f'accuracy-{feature_type}')\n",
    "    plot_history(history, 'Training and Validation ', f'loss-{feature_type}')\n",
    "\n",
    "    # Predict (replace X with actual data)\n",
    "    # predictions = hybrid_nn.predict(X)\n",
    "\n",
    "    # Saving models\n",
    "    model_dict[f'hybrid_{feature_type}'] = hybrid_nn\n",
    "\n",
    "    # Printing and saving training average results\n",
    "    dictionary = print_history_average(history, results_dict, feature_type)\n",
    "\n",
    "    # Evaluate the model (replace X_test, y_test with actual data)\n",
    "    test_loss, test_accuracy = hybrid_nn.evaluate_model(X_test, y_test)\n",
    "    dictionary[f'{feature_type}_test'] = {'loss': test_loss, 'accuracy': test_accuracy}\n",
    "\n",
    "    # Predicting for confusion matrix\n",
    "    y_pred_prob = hybrid_nn.predict(X_test)  # Get probabilities\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class indices\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f'Test : {np.unique(y_test)}')\n",
    "    print(f'Pred : {np.unique(y_pred)}')\n",
    "\n",
    "    # Handling error due to prediction having fewer classes\n",
    "    y_test_unique = np.unique(y_test)\n",
    "    y_pred_unique = np.unique(y_pred)\n",
    "    if len(y_pred_unique) < len(y_test_unique):\n",
    "        # Find the missing classes in y_pred_unique\n",
    "        missing_classes = np.setdiff1d(y_test_unique, y_pred_unique)\n",
    "\n",
    "        # Add the missing classes to y_pred_unique\n",
    "        y_pred_unique = np.concatenate((y_pred_unique, missing_classes))\n",
    "\n",
    "        # Sort y_pred_unique to maintain order (optional)\n",
    "        y_pred_unique = np.sort(y_pred_unique)\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[f\"Actual_{cls}\" for cls in y_test_unique],\n",
    "        columns=[f\"Predicted_{cls}\" for cls in y_pred_unique]\n",
    "    )\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    print(report_df)\n",
    "    report_df.index.name = \"Class\"\n",
    "\n",
    "    cm_dictionary[f'Confusion Matricx {feature_type}'] = cm_df\n",
    "    report_dictionary[f'Classification Report {feature_type}'] = report_df\n",
    "\n",
    "    print('{} : Executing Network for {} Features **************************** : DONE'.format(datetime.now(),\n",
    "                                                                                              feature_type))\n",
    "\n",
    "    return dictionary, model_dict, cm_dictionary, report_dictionary"
   ],
   "id": "a32cc3aa7d4d3bd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calling main",
   "id": "69030ca9e2fced52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Trying multiple learning rates\n",
    "learning_rates = np.linspace(0.001, 0.001, num=1)\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "file_path = ''\n",
    "for instance in range(0, 1):\n",
    "    results_dictionary = {}\n",
    "    model_dictionary = {}\n",
    "    cm_dictionary = {}\n",
    "    report_dictionary = {}\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(pcc_matrix_8s, labels_8s,\n",
    "                                                                                             'PCC_8s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(pcc_matrix_12s, labels_12s,\n",
    "                                                                                             'PCC_12s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(conn_matrix_8s, labels_8s,\n",
    "                                                                                             'CONN_8s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(conn_matrix_12s,\n",
    "                                                                                             labels_12s, 'CONN_12s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(pca_matrix_8s, labels_8s,\n",
    "                                                                                             'PCA_8s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(pca_matrix_12s, labels_12s,\n",
    "                                                                                             'PCA_12s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(sc_matrix_8s, labels_8s,\n",
    "                                                                                             'SC_8s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "    results_dictionary, model_dictionary, cm_dictionary, report_dictionary = execute_network(sc_matrix_12s, labels_12s,\n",
    "                                                                                             'SC_12s',\n",
    "                                                                                             results_dictionary,\n",
    "                                                                                             model_dictionary,\n",
    "                                                                                             learning_rate, epochs,\n",
    "                                                                                             cm_dictionary,\n",
    "                                                                                             report_dictionary)\n",
    "\n",
    "    file_path = save_params_and_results(model_dictionary, results_dictionary, cm_dictionary, report_dictionary,\n",
    "                                        learning_rate, epochs, file_path, instance=instance)"
   ],
   "id": "9b6b0999fac48dcb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
